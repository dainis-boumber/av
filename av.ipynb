{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "av.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iL-jpttJH0fG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "73b8b2c2-4220-44ec-dccf-8f6eac0abbdd"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uyMad0vHItmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "path = Path(base_dir + 'data/bears')\n",
        "\n",
        "dest = path/folder\n",
        "\n",
        "dest.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "metadata": {
        "id": "rk66t78hJC69",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from fastai.text import *\n",
        "from fastai import *\n",
        "import json\n",
        "import html\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sklearn\n",
        "from sklearn import model_selection\n",
        "from functools import partial\n",
        "from collections import Counter, defaultdict\n",
        "from pandas.io.json import json_normalize\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils \n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import dataset, dataloader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from nltk.tokenize import  sent_tokenize\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "import data\n",
        "import joblib\n",
        "from random import shuffle\n",
        "from itertools import permutations, combinations\n",
        "\n",
        "#path = Path(base_dir + 'data/PAN14')\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'Colab Notebooks/'\n",
        "#path = Path(base_dir + '/PAN14/')\n",
        "\n",
        "\n",
        "\n",
        "token_files = base_dir+'PAN14/tokens/'\n",
        "model_files = base_dir+'PAN14/models/'\n",
        "bs=64\n",
        "TRAINDATAPATH = base_dir+\"PAN14/pan14_train_english-essays/\"\n",
        "TESTDATAPATH = base_dir+\"PAN14/pan14_test02_english-essays/\"\n",
        "FNAMES = ['known01','known02','known03','known04','known05', 'unknown']\n",
        "KCOLS=['known01','known02','known03','known04','known05']\n",
        "LABELCOL=\"answer\"\n",
        "UNKOWN=\"unknown\"\n",
        "np.random.seed=42\n",
        "BOD = 'x_bod' # beginning-of-doc tag\n",
        "\n",
        "re1 = re.compile(r'  +')\n",
        "\n",
        "def fixup(x):\n",
        "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
        "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x))\n",
        "\n",
        "def read_dataset(path):\n",
        "    ds=pd.read_json(path+'/truth.json')\n",
        "    ds=json_normalize(ds['problems'])\n",
        "    ds['known01']=None\n",
        "    ds['known02']=None\n",
        "    ds['known03']=None\n",
        "    ds['known04']=None\n",
        "    ds['known05']=None\n",
        "    ds['unknown']=None\n",
        "    ds.set_index('name', drop=True, inplace=True)\n",
        "    ds=ds[['known01','known02','known03','known04','known05', 'unknown', 'answer']]\n",
        "    dirs = []\n",
        "    docs = []\n",
        "\n",
        "    for i, x in enumerate(os.walk(path)):\n",
        "        if i:\n",
        "            for fname in x[2]:\n",
        "                with open(path+dirs[i-1]+'/'+fname, 'r') as f:\n",
        "                    text = f.read().strip()\n",
        "                    doc = ' '.join(sent_tokenize(text)).strip()\n",
        "                    ds.loc[dirs[i-1],fname[:-4]]=doc\n",
        "        else:\n",
        "            dirs = x[1]\n",
        "\n",
        "    return ds\n",
        "\n",
        "def match_unknowns(path):\n",
        "    ds=pd.read_json(path+'/truth.json')\n",
        "    ds=json_normalize(ds['problems'])\n",
        "    ds['known01']=None\n",
        "    ds['known02']=None\n",
        "    ds['known03']=None\n",
        "    ds['known04']=None\n",
        "    ds['known05']=None\n",
        "    ds['unknown']=None\n",
        "    ds.set_index('name', drop=True, inplace=True)\n",
        "    ds=ds[['known01','known02','known03','known04','known05', 'unknown', 'answer']]\n",
        "    dirs = []\n",
        "    docs = []\n",
        "\n",
        "    ds = read_dataset(path)\n",
        "            \n",
        "    grouped=ds.groupby(['unknown'])\n",
        "    dupes=[]\n",
        "    for utext, group in grouped:\n",
        "        if len(group.index) > 1:\n",
        "            dupes.append(group)\n",
        "\n",
        "\n",
        "    newrows=pd.DataFrame(columns=['known01','known02','known03','known04','known05', 'unknown'])\n",
        "    for dupe in dupes:\n",
        "        dupe.reset_index(drop=True, inplace=True)\n",
        "        yes=dupe.loc[dupe.answer == \"Y\"]\n",
        "        yes.reset_index(drop=True, inplace=True)\n",
        "        no=dupe.loc[dupe.answer == \"N\"]\n",
        "        no.reset_index(drop=True, inplace=True)\n",
        "        for col in ['known01','known02','known03','known04','known05']:\n",
        "            if no[col] is not None:\n",
        "                newrows=newrows.append(pd.DataFrame(data={'known01':yes.known01,'known02':yes.known02,\n",
        "                                                          'known03':yes.known03, 'known04':yes.known04,\n",
        "                                                          'known05':yes.known05,'unknown':no[col], \n",
        "                                                          'answer':'N'}), sort=False)\n",
        "    newrows=newrows.dropna(subset=['unknown'])\n",
        "    df = pd.concat([ds, newrows])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def shuffle_text(text, n=4):\n",
        "    pars = [text[i:i+int(len(text)/n)] for i in range(0, len(text), int(len(text)/n))]\n",
        "    shuffle(pars)\n",
        "    return ''.join(pars)\n",
        "\n",
        "\n",
        "def load_doc_pairs(df, augment=False):\n",
        "   \n",
        "    s0s = []\n",
        "    s1s = []\n",
        "    labels = [1 if label == 'Y' else 0 for label in df[LABELCOL].tolist()]\n",
        "    \n",
        "    y=[]\n",
        "    unknowns = df[UNKOWN].tolist()\n",
        "    for i, label in enumerate(labels):\n",
        "        for col in KCOLS:\n",
        "            knowns = df[col].tolist()\n",
        "\n",
        "            s0 = knowns[i]\n",
        "            if s0 is not None:\n",
        "                s1 = unknowns[i]\n",
        "                s0s.append(s0)\n",
        "                s1s.append(s1)\n",
        "                y.append(label)\n",
        "                \n",
        "                if augment:\n",
        "                    s0inflated=shuffle_text(s0)\n",
        "                    s1inflated=shuffle_text(s1)\n",
        "                    s0s.append(s0inflated)\n",
        "                    s1s.append(s1inflated)\n",
        "                    y.append(label)\n",
        "    data = pd.DataFrame(data={\"known\":s0s, \"unknown\":s1s, \"label\":y}) \n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iyzkh8tsVH6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "outputId": "db65829b-ee96-4934-b016-1362a2461eaf"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "df_train = read_dataset(TRAINDATAPATH)\n",
        "df_test = read_dataset(TESTDATAPATH)\n",
        "\n",
        "doc_pairs_train = load_doc_pairs(df_train, False)\n",
        "doc_pairs_test = load_doc_pairs(df_test, False)\n",
        "\n",
        "joblib.dump(doc_pairs_train, f'{model_files}traindf-b.pkl')\n",
        "joblib.dump(doc_pairs_test, f'{model_files}testdf-b.pkl')\n",
        "doc_pairs_train['is_valid'] = False\n",
        "doc_pairs_test['is_valid'] = True\n",
        "df = pd.concat([doc_pairs_train,doc_pairs_test]).sample(frac=1)\n",
        "joblib.dump(df, 'data.pkl')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f3efa3cc7111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINDATAPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTESTDATAPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc_pairs_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_doc_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a16fd0072dc0>\u001b[0m in \u001b[0;36mread_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/truth.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'problems'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'known01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m             )\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'frame'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 793\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    794\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             decoded = dict((str(k), v)\n",
            "\u001b[0;31mValueError\u001b[0m: Expected object or value"
          ]
        }
      ]
    }
  ]
}