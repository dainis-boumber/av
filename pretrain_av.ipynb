{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretrain_av.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dainis-boumber/av/blob/master/pretrain_av.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZNmeh0h4UYSR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ULMFiT + Siamese Network for Sentence Vectors\n",
        "## Part One: Tokenizing\n",
        "This notebook will tokenize the sentences from the SNLI dataset for use in the next notebook\n",
        "\n",
        "### You must have the fastai library installed"
      ]
    },
    {
      "metadata": {
        "id": "91TjIhFCUYSS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "from ipyexperiments import *\n",
        "from fastai.text import *\n",
        "from fastai import *\n",
        "import json\n",
        "import html\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sklearn\n",
        "from sklearn import model_selection\n",
        "from functools import partial\n",
        "from collections import Counter, defaultdict\n",
        "from pandas.io.json import json_normalize\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils \n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import dataset, dataloader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "import data\n",
        "import joblib\n",
        "\n",
        "token_files = './data/PAN14/tokens/'\n",
        "model_files = './data/PAN14/models/'\n",
        "TRAINDATAPATH = \"./data/PAN14/pan14_train_english-essays/\"\n",
        "TESTDATAPATH = \"./data/PAN14/pan14_test01_english-essays/\"\n",
        "FNAMES = ['known01','known02','known03','known04','known05', 'unknown']\n",
        "KCOLS=['known01','known02','known03','known04','known05']\n",
        "LABELCOL=\"answer\"\n",
        "UNKOWN=\"unknown\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8QFCt7QUYSW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ! wget https://github.com/briandw/SiameseULMFiT/releases/download/1/data.zip\n",
        "# ! unzip ./data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O1A7heLLUYSZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BOD = 'x_bod' # beginning-of-doc tag\n",
        "\n",
        "re1 = re.compile(r'  +')\n",
        "\n",
        "def fixup(x):\n",
        "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
        "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x))\n",
        "\n",
        "def read_dataset(path):\n",
        "    ds=pd.read_json(path+'/truth.json')\n",
        "    ds=json_normalize(ds['problems'])\n",
        "    ds['known01']=None\n",
        "    ds['known02']=None\n",
        "    ds['known03']=None\n",
        "    ds['known04']=None\n",
        "    ds['known05']=None\n",
        "    ds['unknown']=None\n",
        "    ds.set_index('name', drop=True, inplace=True)\n",
        "    ds=ds[['known01','known02','known03','known04','known05', 'unknown', 'answer']]\n",
        "    dirs = []\n",
        "    docs = []\n",
        "\n",
        "    for i, x in enumerate(os.walk(path)):\n",
        "        if i:\n",
        "            for fname in x[2]:\n",
        "                with open(path+dirs[i-1]+'/'+fname, 'r') as f:\n",
        "                    text = f.read().strip()\n",
        "                    doc = BOD.join(sent_tokenize(text))\n",
        "                    docs.append(doc)\n",
        "                    ds.loc[dirs[i-1],fname[:-4]]=doc\n",
        "        else:\n",
        "            dirs = x[1]\n",
        "\n",
        "    return ds, docs\n",
        "\n",
        "def match_unknowns(path):\n",
        "    ds=pd.read_json(path+'/truth.json')\n",
        "    ds=json_normalize(ds['problems'])\n",
        "    ds['known01']=None\n",
        "    ds['known02']=None\n",
        "    ds['known03']=None\n",
        "    ds['known04']=None\n",
        "    ds['known05']=None\n",
        "    ds['unknown']=None\n",
        "    ds.set_index('name', drop=True, inplace=True)\n",
        "    ds=ds[['known01','known02','known03','known04','known05', 'unknown', 'answer']]\n",
        "    dirs = []\n",
        "    docs = []\n",
        "\n",
        "    ds, _ = read_dataset(path)\n",
        "            \n",
        "    grouped=ds.groupby(['unknown'])\n",
        "    dupes=[]\n",
        "    for utext, group in grouped:\n",
        "        if len(group.index) > 1:\n",
        "            dupes.append(group)\n",
        "\n",
        "\n",
        "    newrows=pd.DataFrame(columns=['known01','known02','known03','known04','known05', 'unknown'])\n",
        "    for dupe in dupes:\n",
        "        dupe.reset_index(drop=True, inplace=True)\n",
        "        yes=dupe.loc[dupe.answer == \"Y\"]\n",
        "        yes.reset_index(drop=True, inplace=True)\n",
        "        no=dupe.loc[dupe.answer == \"N\"]\n",
        "        no.reset_index(drop=True, inplace=True)\n",
        "        for col in ['known01','known02','known03','known04','known05']:\n",
        "            if no[col] is not None:\n",
        "                newrows=newrows.append(pd.DataFrame(data={'known01':yes.known01,'known02':yes.known02,\n",
        "                                                          'known03':yes.known03, 'known04':yes.known04,\n",
        "                                                          'known05':yes.known05,'unknown':no[col], \n",
        "                                                          'answer':'N'}), sort=False)\n",
        "    newrows=newrows.dropna(subset=['unknown'])\n",
        "    df = pd.concat([ds, newrows])\n",
        "    for col in FNAMES:\n",
        "        \n",
        "        docs.extend(df[col].tolist())\n",
        "    docs=[d for d in docs if d is not None]\n",
        "    return df, docs\n",
        "\n",
        "df_train, docs = match_unknowns(TRAINDATAPATH)\n",
        "\n",
        "df_test, _ = read_dataset(TESTDATAPATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZX_JJLyjUYSc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_sentence_pairs(df):\n",
        "   \n",
        "    s0s = []\n",
        "    s1s = []\n",
        "    labels = [1 if label == 'Y' else 0 for label in df[LABELCOL].tolist()]\n",
        "    \n",
        "    y=[]\n",
        "    unknowns = df[UNKOWN].tolist()\n",
        "    for i, label in enumerate(labels):\n",
        "        for col in KCOLS:\n",
        "            knowns = df[col].tolist()\n",
        "\n",
        "            s0 = knowns[i]\n",
        "            if s0 is not None:\n",
        "                s1 = unknowns[i]\n",
        "                s0s.append(s0)\n",
        "                s1s.append(s1)\n",
        "                y.append(label)\n",
        "    pairs=pd.DataFrame(data={\"known\":s0s, \"unknown\":s1s, \"label\":y})\n",
        "    return pairs  \n",
        "\n",
        "sentence_pairs_train = load_sentence_pairs(df_train)\n",
        "sentence_pairs_val = load_sentence_pairs(df_test)\n",
        "sentence_pairs_test = load_sentence_pairs(df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VQxSVlicUYSf",
        "colab_type": "code",
        "colab": {},
        "outputId": "1f5b035d-4b90-4ad3-c440-2cb960e2d70f"
      },
      "cell_type": "code",
      "source": [
        "joblib.dump(sentence_pairs_train, f'{model_files}traindf.pkl')\n",
        "joblib.dump(sentence_pairs_val, f'{model_files}valdf.pkl')\n",
        "joblib.dump(sentence_pairs_test, f'{model_files}testdf.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./data/PAN14/models/testdf.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "uAKm_UEuUYSk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence_pairs_train['label']=0\n",
        "sentence_pairs_val['label']=0\n",
        "sentence_pairs_test['label']=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0e6fhmNUYSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Language model data\n",
        "data_lm = TextLMDataBunch.from_df(model_files, sentence_pairs_train, sentence_pairs_val, sentence_pairs_test,\n",
        "                                  text_cols=['known', 'unknown'], label_cols=['label'], mark_fields=True)\n",
        "data_lm.save()                              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OH6tAEsnUYSq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm = TextLMDataBunch.load(model_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7hlEqcmpUYSu",
        "colab_type": "code",
        "colab": {},
        "outputId": "001641f8-8e96-4379-bf16-37f1f7350c64"
      },
      "cell_type": "code",
      "source": [
        "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103_1, drop_mult=0.5)\n",
        "learn.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 00:44 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>4.307320</th>\n",
              "    <th>4.082573</th>\n",
              "    <th>0.250004</th>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "a1xfLHl8UYSx",
        "colab_type": "code",
        "colab": {},
        "outputId": "f08c05e9-b5b5-4680-f25a-4ba938ff5396"
      },
      "cell_type": "code",
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(1, 1e-3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 00:54 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>3.792723</th>\n",
              "    <th>3.911833</th>\n",
              "    <th>0.271624</th>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Ao7W3VdvUYS1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save_encoder('ft_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o3KnMgCYUYS4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the language model training set"
      ]
    }
  ]
}